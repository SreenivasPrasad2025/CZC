# -*- coding: utf-8 -*-
"""CrimeZoneClassifier_FinalCopy.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Qh33F97dgwbevKC5PICafqulZMeLFmO4
"""

pip install --quiet hdbscan

pip install --quiet pmdarima

pip install --quiet matplotlib basemap folium

!pip install --quiet  ydata-profiling

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import warnings

warnings.simplefilter(action='ignore', category=FutureWarning)

data = pd.read_csv('Crime_Data_from_2020_to_Present_20240330.csv')

# Display the first few rows to understand its structure
print(data.head())

# reading the feature descriptions
ft_desc_df = pd.read_csv("feature_descr.txt", sep=": ", names=["name", "description"], index_col="name", engine='python').apply(lambda x: x.str.strip())

ft_desc_df.sample(10)

# rename dataset's columns to improve readability
rename_dict = ft_desc_df.squeeze(axis=1).to_dict()
data_df = data.rename(rename_dict, axis="columns")
pd.set_option('display.max_columns', None)
data_df.sample(3)

data.info()

# Summary of the dataset
print(data.describe())

# Check for missing values
print(data.isnull().sum())

"""**Strong Positive Correlations:** Cells with a correlation close to 1.00 indicate variables that move in the same direction. For example, 'Rpt Dist No' (Report District Number) and 'Part 1-2' seem to have a strong positive correlation, suggesting that as one increases, the other tends to increase as well.

**Strong Negative Correlations:** Cells with a correlation close to -1.00 indicate variables that move in opposite directions. However, in this heatmap, there don't appear to be any pairs of variables with a strong negative correlation.

**No/Weak Correlations:** Cells with a correlation close to 0 indicate no or a very weak relationship between the variables. For instance, 'DR_NO' (which could be a report number) and 'TIME_OCC' (possibly time of occurrence) show very little correlation with the other variables.

**Diagonal:** The diagonal from the top left to the bottom right shows perfect positive correlation (1.00) because it represents each variable's correlation with itself.

**High Correlation Pairs:** There are a few pairs of variables with a notably high correlation, such as: 'Part 1-2' with 'Rpt Dist No'
'Crm Cd 1' with 'Part 1-2' and 'Rpt Dist No'
Other 'Crm Cd' (Crime Codes) also show moderate correlations with each other, which could suggest a relationship between different categories of crimes.

**Numerical Columns:** Include DR_NO, TIME OCC, AREA, Rpt Dist No, Part 1-2, Crm Cd, Vict Age, Premis Cd, Weapon Used Cd, etc.

**Categorical Columns:** Include Date Rptd, DATE OCC, AREA NAME, Crm Cd Desc, Vict Sex, Vict Descent, Premis Desc, Weapon Desc, Status, Status Desc, LOCATION, Cross Street, etc.

**Missing Values:** Several columns have missing values, most notably Mocodes, Vict Sex, Vict Descent, Premis Desc, Weapon Used Cd, Weapon Desc, Crm Cd 2, Crm Cd 3, and Crm Cd 4. The handling of these missing values will depend on the analysis focus and could range from imputation to removal, depending on their importance.
"""

# Strata based on Geographical Areas
strata_column = 'AREA NAME'
sample_size = 10000

# Compute the proportionate size of each stratum
strata_sizes = (data[strata_column].value_counts() / len(data)).mul(sample_size).round().astype(int)
stratified_sample = pd.DataFrame()

# Stratified sampling from each stratum
for stratum, size in strata_sizes.items():
    stratum_sample = data[data[strata_column] == stratum].sample(n=size, random_state=1)
    stratified_sample = pd.concat([stratified_sample, stratum_sample])

stratified_sample.info()

original_proportions = data['AREA NAME'].value_counts(normalize=True)
sample_proportions = stratified_sample['AREA NAME'].value_counts(normalize=True)

stratified_sample.head()

# Comparing the proportion of the original and sampled data
comparison_df = pd.DataFrame({'Original': original_proportions, 'Sample': sample_proportions})
print(comparison_df)

# Visualising the Original vs Sample proportions
comparison_df.plot(kind='bar', figsize=(14, 7))
plt.title('Comparison of Area Name Proportions: Original vs. Sample')
plt.xlabel('Area Name')
plt.ylabel('Proportion')
plt.xticks(rotation=45)
plt.legend(['Original', 'Sample'])
plt.tight_layout()
plt.show()

# Profilng Report
from ydata_profiling import ProfileReport
profile = ProfileReport(data, title="Pandas Profiling Report",explorative=True)

profile

# Save to a file
profile.to_file("data_report.html")

# Continuous Feature report
contin_feat_names = data[data.select_dtypes("number").columns]
contin_feat_names
category_feat_names=data[data.select_dtypes(exclude="number").columns]
category_feat_names


def continuous(df):
    cardinality = df.nunique()
    missing = df.isna().sum()/len(data)*100
    missing_df=pd.DataFrame()
    missing_df['missing']=missing
    cardinality_df=pd.DataFrame()
    cardinality_df['cardinality']=cardinality
    continuous = df.describe(include=['number'])
    continuous = pd.concat([continuous, missing_df.T])
    continuous = pd.concat([continuous, cardinality_df.T])
    return continuous
continuous_feat_report=continuous(contin_feat_names)
continuous_feat_report

# Categorical Feature Report

def categorical_report(df):
    # Calculate basic metrics
    count = df.count()
    missing = df.isna().sum()
    miss_percent = (missing / len(df)) * 100
    cardinality = df.nunique()

    # Initialize the report DataFrame
    report_df = pd.DataFrame(index=df.columns)
    report_df['Count'] = count
    report_df['Miss %'] = miss_percent
    report_df['Card.'] = cardinality

    # Mode, Mode Frequency, and Mode Percentage
    mode = df.mode().iloc[0]
    mode_freq = df.apply(lambda x: x.value_counts().iloc[0])
    mode_percent = (mode_freq / count) * 100

    # Adding Mode metrics to the report
    report_df['Mode'] = mode
    report_df['Mode Freq'] = mode_freq
    report_df['Mode %'] = mode_percent

    # Calculating 2nd Mode, its Frequency, and Percentage
    def second_mode(x):
        if x.value_counts().size > 1:
            return x.value_counts().index[1]
        return None

    def second_mode_freq(x):
        if x.value_counts().size > 1:
            return x.value_counts().iloc[1]
        return None

    second_mode = df.apply(second_mode)
    second_mode_freq = df.apply(second_mode_freq)
    second_mode_percent = (second_mode_freq / count) * 100

    # Adding 2nd Mode metrics to the report
    report_df['2nd Mode'] = second_mode
    report_df['2nd Mode Freq'] = second_mode_freq
    report_df['2nd Mode %'] = second_mode_percent

    return report_df

# Assuming 'category_feat_names' contains the categorical features of your dataframe 'data'
categorical_feat_report = categorical_report(category_feat_names)
print(categorical_feat_report)

data = stratified_sample

# 1. Handle Missing Values
# Drop columns with high percentage of missing values
data.drop(['Crm Cd 2', 'Crm Cd 3', 'Crm Cd 4'], axis=1, inplace=True)
data.drop(['Date Rptd'],axis=1,inplace=True)
# Fill missing values for other columns
numeric_cols = data.select_dtypes(include=['float64', 'int64']).columns
data[numeric_cols] = data[numeric_cols].fillna(data[numeric_cols].mode().iloc[0])

# For categorical columns, use the mode (most frequent value)
categorical_cols = data.select_dtypes(include=['object', 'category']).columns
data[categorical_cols] = data[categorical_cols].fillna(data[categorical_cols].mode().iloc[0])

# 2. Feature Extraction from Datetime Fields
# Extract day of the week and time of the day
data['DATE OCC'] = pd.to_datetime(data['DATE OCC'], format='%m/%d/%Y %I:%M:%S %p')

# Now extract features from the datetime column
data['Day of Week'] = data['DATE OCC'].dt.day_name()
data['TIME OCC'] = data['TIME OCC'].astype(str).str.zfill(4)
#data['TIME OCC'] = pd.to_datetime(data['TIME OCC'], format='%H%M').dt.time

data['TIME OCC'] = pd.to_datetime(data['TIME OCC'], format='%H%M')
# Extracting time for display or reporting purposes
data['Display Time'] = data['TIME OCC'].dt.time


# 3. Remove Duplicates
data = data.drop_duplicates()

# Define a function to handle outliers in a column
def handle_outliers(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Cap values below the lower bound and above the upper bound
    df[column] = df[column].clip(lower=lower_bound, upper=upper_bound)

# Apply the function to each numeric column
numeric_cols = data.select_dtypes(include=['float64', 'int64']).columns
for col in numeric_cols:
    handle_outliers(data, col)

# Save the cleaned data
cleaned_file_path = 'Cleaned_CrimeData.csv'
data.to_csv(cleaned_file_path, index=False)

cleaned_file_path

"""**Approach Used:**

1. Dropped the columns with high percentage of missing values(Crm Cd 2', 'Crm Cd 3', 'Crm Cd 4)

2. Fill missing values for other columns

   2.1. **Mocodes (Modus Operandi codes):** These codes identify specific methods or behaviors associated with the commission of a crime. Each code corresponds to a different modus operandi. The numeric representation does not imply any ordinal or quantitative relationship between different modus operandi; instead, each number is a unique identifier for a particular method or behavior.

   2.2. **Weapon Used Cd (Weapon Used Code):** This variable indicates the type of weapon used in a crime incident, if any. Each code corresponds to a different type of weapon. Like "Mocodes", these numeric codes serve as identifiers for categories (types of weapons) and do not imply any quantitative measurement or inherent order.

  2.3. **Premis Cd (Premise Code):** This code represents the type of location or premises where a crime occurred, such as a home, store, or public street. Each unique code identifies a different type of premises. The numeric codes are labels for categorizing the location types and do not have a quantitative significance.

3. For certain types of categorical variables, like "Weapon Used Cd" or "Premis Cd" or "Mocodes", the mode may indeed represent a significant portion of the data, making it a sensible choice for imputation. For example, if a large proportion of crimes occur on the street, using the mode to impute missing "Premis Cd" values with 'street' might accurately reflect reality.

4. For categorical columns, use the mode (most frequent value)
"""

data.isnull().sum()

"""**DR_NO:** Being a unique report number, it is likely to have a large range of unique values, but it's not represented in the boxplot because it's not a meaningful numerical feature for analysis.

**AREA:** The numeric code representing the area seems to have a relatively small range of values. The distribution is tight as indicated by the compact box, suggesting most of the crime reports come from a limited number of areas or that area codes are closely numbered.

**Part 1-2:** If this is a binary or categorical variable (indicating whether the crime is a Part 1 or Part 2 offense), the boxplot suggests two distinct groups, likely corresponding to each category. However, if these are indeed categories, a boxplot may not be the appropriate visualization tool.

**Crm Cd:** This appears to have a wide range, but without knowing how these codes are assigned, it's difficult to interpret the spread. It could indicate a wide variety of crime types in the dataset.

**Vict Age:** Victim ages show a range with some potential outliers on the lower end (possible given the minimum age is likely to be close to zero). The box itself suggests a concentration of crime in certain age groups.

**Premis Cd:** The premise code, like area, appears to be tightly grouped, which might suggest crimes occur in a limited variety of location types.

**Weapon Used Cd:** If this is categorical, the plot indicates that there might be a common type of weapon used across many incidents, with some rarer types (potential outliers) as well.

**LAT and LON:** The latitude and longitude coordinates show a range that is consistent with a specific geographic location. The spread and potential outliers would be points of interest when mapping crime locations to identify hotspots.

**Day of Week:** This variable is likely categorical (with values 1-7 representing the days of the week), and it's not ideal to represent it with a boxplot. A bar chart would be more suitable to show the frequency of crimes on different days of the week.

Regarding the scaling of the data:

For clustering, it's often important to scale the data, especially when the

*   algorithm relies on distance metrics (like K-means or DBSCAN). This ensures that variables with large ranges don't disproportionately influence the clustering results.

* For latitude and longitude data used in geographical clustering, scaling might not be necessary as they are already in comparable units (degrees).

* For categorical data like 'Part 1-2', 'Day of Week', or 'Weapon Used Cd', scaling is not applicable. Instead, these should be handled via encoding techniques if they're to be used in clustering algorithms.

"""

import seaborn as sns
import matplotlib.ticker as ticker

# Distribution of Crimes over time (monthly)
data['YearMonth'] = data['DATE OCC'].dt.to_period('M')
crimes_monthly = data['YearMonth'].value_counts().sort_index()

# Distribution of Victim Age (excluding unrealistic ages)
victim_age_dist = data[data['Vict Age'] > 0]['Vict Age']

# Top 10 Crime Types
top_crime_types = data['Crm Cd Desc'].value_counts().head(10)

# Prepare for geographic distribution plot (excluding outliers)
geo_data = data[(data['LAT'] > 33) & (data['LON'] < -117)]

# Plotting
fig, axs = plt.subplots(2, 2, figsize=(18, 12))
ticker_spacing= 3

# Monthly Crimes
axs[0, 0].xaxis.set_major_locator(ticker.MultipleLocator(ticker_spacing))
axs[0, 0].plot(crimes_monthly.index.astype(str), crimes_monthly.values, marker='o', linestyle='-')
axs[0, 0].set_title('Crimes Over Time')
axs[0, 0].set_xlabel('Month')
axs[0, 0].set_ylabel('Number of Crimes')
axs[0, 0].tick_params(axis='x', rotation=45)

# Victim Age Distribution
sns.histplot(victim_age_dist, bins=30, kde=True, ax=axs[0, 1])
axs[0, 1].set_title('Distribution of Victim Age')
axs[0, 1].set_xlabel('Age')
axs[0, 1].set_ylabel('Frequency')

# Top 10 Crime Types
top_crime_types.plot(kind='barh', ax=axs[1, 0])
axs[1, 0].set_title('Top 10 Crime Types')
axs[1, 0].set_xlabel('Frequency')
axs[1, 0].set_ylabel('Crime Type')

# Geographic Distribution of Crimes
axs[1, 1].scatter(geo_data['LON'], geo_data['LAT'], alpha=0.2)
axs[1, 1].set_title('Geographic Distribution of Crimes')
axs[1, 1].set_xlabel('Longitude')
axs[1, 1].set_ylabel('Latitude')

plt.tight_layout()
plt.show()

"""**Crimes Over Time:**

This line graph shows the number of crimes over a period of time, which seems to be measured monthly. There is a noticeable trend or pattern in crime occurrence over time, but the specific time frame is not legible. The last data point shows a sharp decline, which could be due to incomplete data for the most recent month or a sudden drop in crime incidents.

**Distribution of Victim Age:**

The histogram depicts the distribution of victims' ages. It is bimodal with two peaks, suggesting that there are two age groups that are more frequently victimized than others. The data appears to be skewed right, indicating that younger individuals are more often victims compared to older individuals.

**Top 10 Crime Types:**

This horizontal bar chart shows the most common crime types. The specific crime types are not entirely legible, but it's clear that the most frequent crime type far exceeds others in frequency. This chart is valuable for understanding which crimes are most prevalent in the area being studied.

**Geographic Distribution of Crimes (Scatter Plot):**

The plot labeled "Crime Incidents" is a scatter plot showing geographic data points, likely representing individual crimes, based on their longitude (LON) and latitude (LAT) coordinates.
The plot reveals a heavy concentration of incidents in certain areas, with a high density of points that almost forms a solid shape. This indicates regions with a high frequency of reported crimes.
There are also some sparse points indicating isolated incidents further away from the main cluster, which could be outliers or areas with less crime.
The data appears to be from a city or region given the shape of the distribution and the range of the coordinates.
"""

import folium
import random
from folium import Choropleth, Circle, Marker
from folium.plugins import HeatMap, MarkerCluster

m_1 = folium.Map(location=[34.051066, -118.244205])
for idx,row in data.iterrows():
    Marker([row['LAT'],row['LON']]).add_to(m_1)
plugin_geocoder = folium.plugins.Geocoder()
plugin_geocoder.add_to(m_1)
m_1

"""The map generated by this code would visually represent the geographical distribution of crime data points from the dataset.
It is an interactive map with markers placed at specific locations based on latitude (LAT) and longitude (LON) values from the data DataFrame. The folium.Map function creates a map object centered at a given location, in this case, coordinates corresponding to somewhere in Los Angeles,
Each marker on the map corresponds to a crime incident, which can help in understanding the concentration and spread of crime across different areas.
Such a visual analysis tool could be used by law enforcement to identify hotspots of criminal activity and allocate resources accordingly.
"""

import time
from tqdm import tqdm


m_2 = folium.Map(location=[34.051066, -118.244205],tiles='cartodbpositron',zoom_start = 13)
mc = MarkerCluster()
for idx,row in data.iterrows():
    mc.add_child(Marker([row['LAT'],row['LON']]))

m_2.add_child(mc)

m_2

for i in tqdm(range(100)):
    time.sleep(0.05)

import folium
from folium.plugins import MarkerCluster
from tqdm import tqdm

# Create an empty folium map
m_2 = folium.Map(location=[34.051066, -118.244205], tiles='cartodbpositron', zoom_start=13)
mc = MarkerCluster()

# Create a tqdm progress bar
progress_bar = tqdm(total=len(data))

for idx, row in data.iterrows():
    mc.add_child(folium.Marker([row['LAT'], row['LON']]))

    # Update the progress bar
    progress_bar.update(1)

# Add MarkerCluster to the folium map
m_2.add_child(mc)

# Close the tqdm progress bar
progress_bar.close()

# Display the folium map
m_2

"""This interactive map generated by Folium and displayed with a MarkerCluster shows the distribution of reported crime locations in a part of Los Angeles. The map is centered around a specific point, likely a central location in the city, and uses a zoom level that provides a broad overview while still allowing for individual markers to be distinguished.

The numbers on the markers indicate the count of crimes reported at or near each location, with the color likely representing the density or perhaps the severity of the incidents. Areas with a higher concentration of crime are represented by larger, more vividly colored markers, while areas with fewer reported incidents have smaller, lighter-colored markers.

Analyzing this map provides several insights:

**Crime Distribution:** There appears to be a higher concentration of crime in the central downtown area, as indicated by the large marker with "933". This could suggest that the urban center has a higher crime rate, which is not uncommon in major cities.

**Crime Hotspots:** Certain neighborhoods, like Koreatown ("416"), Westlake ("355"), and Pico-Union ("216"), also show higher crime counts. These could be identified as hotspots that may require more attention from law enforcement.

**Varied Crime Rates:** Surrounding neighborhoods exhibit significantly varying crime rates. For instance, areas towards the north, like West Hollywood ("539"), show different patterns than more central areas like Downtown LA.
"""

m_3 = folium.Map(location=[34.051066, -118.244205],tiles='cartodbpositron',zoom_start = 13)

HeatMap(data = data[['LAT','LON']],radius = 13).add_to(m_3)

m_3

"""It is a heatmap generated using Folium's HeatMap plugin, overlaid on a map of a section of Los Angeles.

A heatmap is a data visualization tool that shows the density of points or occurrences on a map with colors indicating the level of density. In the context of urban crime analysis:

Areas with a high density of crimes are typically indicated by warmer colors (reds and oranges), suggesting "hotspots" where crime is more frequent. In this heatmap, there appears to be a concentration of crime in central Los Angeles, with pockets of heightened activity in surrounding areas like West Hollywood and Culver City.

Lower density areas are indicated by cooler colors (greens and blues), which suggest fewer crimes or a lower frequency of incidents. These could be residential areas, areas with less foot traffic, or zones with effective crime prevention measures in place.

The key points of analysis for this heatmap would be:

**Identification of Crime Hotspots:** The map clearly shows where the highest concentration of crimes occurs, aiding law enforcement in identifying areas that may require more attention or resources.
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Generating basic crime trends and distributions
sns.countplot(x='Day of Week', data=data)
plt.title('Crime Count by Day of Week')
plt.xticks(rotation=45)
plt.show()

"""**Crime Count by Day of Week (Bar Chart):**

The second plot is a bar chart displaying the count of crimes for each day of the week.
The x-axis represents the days of the week, although the specific days are not legible in the image.
The y-axis shows the count of crimes, which indicates the frequency of crimes on each day.
The bars seem relatively even in height, suggesting that the crime count does not fluctuate significantly by day of week.
"""

# Count the number of crimes in each area.
crime_counts = data.groupby('AREA NAME')['DR_NO'].count().reset_index(name='CrimeCount')

# Merge the crime counts back with the original data.
data = data.merge(crime_counts, on='AREA NAME', how='left')

# Define thresholds for high, medium, and low incidence zones.
# Calculate the percentiles
low_threshold = data['CrimeCount'].quantile(0.25)
medium_threshold = data['CrimeCount'].quantile(0.5)
high_threshold = data['CrimeCount'].quantile(0.75)

# Apply the thresholds to categorize the 'RiskLevel'
data['RiskLevel'] = pd.cut(
    data['CrimeCount'],
    bins=[0, low_threshold, medium_threshold, high_threshold, float('inf')],
    labels=['Low', 'Medium', 'High', 'Very High'],
    include_lowest=True
)

data[['AREA NAME', 'CrimeCount', 'RiskLevel']]

import matplotlib.pyplot as plt
import seaborn as sns

# Set the aesthetic style of the plots
sns.set_style("whitegrid")

# Count the number of areas in each risk level
risk_level_counts = data['RiskLevel'].value_counts().sort_index()

## Bar Chart ##
plt.figure(figsize=(10, 6))
sns.barplot(x=risk_level_counts.index, y=risk_level_counts.values, palette='viridis')
plt.title('Number of Areas by Risk Level')
plt.xlabel('Risk Level')
plt.ylabel('Number of Areas')
plt.xticks(rotation=45)  # Rotate labels if needed
plt.show()

"""**Low Risk:** This category has the highest number of areas. The tall bar suggests that a large portion of the areas surveyed fall under the 'Low Risk' category, indicating that they have lower crime rates or incidents.

**Medium Risk:** The number of areas in this risk category drops compared to the 'Low Risk' category but still represents a significant count. This could indicate areas where crime exists at a moderate level.

**High Risk:** There is a slight increase in the number of areas compared to the 'Medium Risk' category. This suggests that there are more areas where the crime rate or incidents are relatively high.

**Very High Risk:** The number of areas in this category is the largest next to the 'Low Risk' category. This is a significant finding, as it suggests that there are almost as many 'Very High Risk' areas as there are 'Low Risk' areas.
"""

plt.figure(figsize=(14, 10))
sns.barplot(x='AREA NAME', y='CrimeCount', hue='RiskLevel', data=data, dodge=False)

# Set the plot title and labels
plt.title('Crime Count and Risk Level by Area')
plt.xlabel('Area Name')
plt.ylabel('Crime Count')

# Rotate x-axis labels for better readability
plt.xticks(rotation=45, ha='right')


plt.legend(title='Risk Level', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

"""****EXPECTED FUNCTIONALITY****

Clustering using:
1. DBSCAN
2. HDBSCAN
3. K-MEANS
"""

from sklearn.cluster import DBSCAN

#data.drop(['CrimeCount', 'RiskLevel'],axis=1, inplace=True)
coords = data[['LAT', 'LON']].values

# Convert latitude and longitude to radians for use in haversine metric
coords_in_radians = np.radians(coords)

# DBSCAN clustering
db = DBSCAN(eps=0.00026, min_samples=30, algorithm='ball_tree', metric='haversine').fit(coords_in_radians)

# Assign cluster labels to the original data
data['cluster'] = db.labels_

# Count the number of crimes in each cluster, excluding noise points
crime_counts_per_cluster = data[data['cluster'] != -1].groupby('cluster').size()

# Calculate quantiles to define risk levels
low_risk_threshold = crime_counts_per_cluster.quantile(0.25)
medium_risk_threshold = crime_counts_per_cluster.quantile(0.5)
high_risk_threshold = crime_counts_per_cluster.quantile(0.75)

# Function to assign risk level based on crime count
def get_risk_level(count):
    if count <= low_risk_threshold:
        return 'Low'
    elif count <= medium_risk_threshold:
        return 'Medium'
    elif count <= high_risk_threshold:
        return 'High'
    else:
        return 'Very High'

# Assign a risk level to each cluster
risk_levels = crime_counts_per_cluster.map(get_risk_level)
data['risk_level'] = data['cluster'].map(risk_levels)

# Replace risk level for noise points if needed
data.loc[data['cluster'] == -1, 'risk_level'] = 'Noise'

# Display the DataFrame
print(data[['LAT', 'LON', 'cluster', 'risk_level']])

from sklearn.metrics import silhouette_score
from sklearn.cluster import DBSCAN
import numpy as np

eps_values = np.arange(0.0001, 0.001, 0.0001)
min_samples_values = range(5, 50, 5)

best_score = -1
best_params = {'eps': None, 'min_samples': None}

for eps in eps_values:
    for min_samples in min_samples_values:
        db = DBSCAN(eps=eps, min_samples=min_samples, metric='haversine').fit(coords_in_radians)
        labels = db.labels_

        if len(set(labels)) - (1 if -1 in labels else 0) > 1:
            score = silhouette_score(coords_in_radians, labels)
            if score > best_score:
                best_score = score
                best_params['eps'] = eps
                best_params['min_samples'] = min_samples

print(f"Best silhouette score: {best_score}")
print(f"Best parameters: eps={best_params['eps']}, min_samples={best_params['min_samples']}")

import matplotlib.pyplot as plt
import matplotlib.colors as mcolors

# Ensure that 'lat' and 'lon' are in the correct data type
data['LAT'] = pd.to_numeric(data['LAT'])
data['LON'] = pd.to_numeric(data['LON'])

# Create a color map based on the number of clusters plus noise
colors = plt.cm.rainbow(np.linspace(0, 1, len(crime_counts_per_cluster) + 1))

# Map each risk level to a color
risk_level_colors = {
    'Low': 'green',
    'Medium': 'yellow',
    'High': 'orange',
    'Very High': 'red',
    'Noise': 'grey'
}

# Assign colors to each data point based on risk level
data['color'] = data['risk_level'].apply(lambda x: risk_level_colors[x])

# Plot
plt.figure(figsize=(12, 8))

# Scatter plot with clustering
scatter = plt.scatter(data['LON'], data['LAT'], c=data['color'], alpha=0.5)

# Legend
handles = [plt.Line2D([0], [0], marker='o', color='w', label=risk,
                       markerfacecolor=risk_level_colors[risk], markersize=10)
           for risk in risk_level_colors]
plt.legend(title='Risk Level', handles=handles, loc='upper right')

# Title and labels
plt.title('DBSCAN Clustering of Crimes by Risk Level')
plt.xlabel('Longitude')
plt.ylabel('Latitude')

# Show plot
plt.show()

"""This is a scatter plot showing the clustering of crimes by risk level using the DBSCAN algorithm. Each point represents a crime event, with its position corresponding to geographical coordinates (latitude and longitude).

**Clusters by Color:** The plot uses color to distinguish between different risk levels.

**Red:** Represents areas with a "Very High" risk level. These areas have the highest density of points, indicating a high concentration of crime events. The spread and size of the red clusters suggest that these areas may be significant hotspots for criminal activity.

**Orange:** Indicates "High" risk areas with a moderately high density of points. These areas are also fairly widespread, showing a considerable amount of criminal activity.

**Yellow:** Designates "Medium" risk areas. There are fewer points in these clusters, indicating a lower occurrence of crime compared to red and orange areas.

**Green:** Marks "Low" risk areas with very few points, suggesting these areas experience relatively few crimes.

**Gray:** Denotes "Noise", which in DBSCAN refers to outliers or points that do not fit well into any cluster. These could be one-off incidents or scattered, isolated crimes.

**Spatial Distribution:** The clusters are unevenly distributed across the geographical space. There is a large, dense cluster of high-risk (red) crimes, which could point to systemic issues in that region, such as economic, social, or policing challenges.

**Risk Level Implications:** The concentration of high and very high-risk areas might require targeted law enforcement intervention, community support, and policy action.

**Noise Interpretation:** The noise points suggest either random occurrences of crime that do not follow a pattern or possibly data errors or anomalies.
"""

from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score

# Make sure to exclude noise points before calculating the metrics
filtered_data = coords_in_radians[data['cluster'] != -1]
filtered_labels = data['cluster'][data['cluster'] != -1]

# Calinski-Harabasz Index
calinski_harabasz = calinski_harabasz_score(filtered_data, filtered_labels)
print(f'Calinski-Harabasz Index: {calinski_harabasz}')

# Davies-Bouldin Index
davies_bouldin = davies_bouldin_score(filtered_data, filtered_labels)
print(f'Davies-Bouldin Index: {davies_bouldin}')

from sklearn.neighbors import NearestNeighbors
import matplotlib.pyplot as plt

coords = data[['LAT', 'LON']].values

# Convert latitude and longitude to radians for use in haversine metric
coords_in_radians = np.radians(coords)
# Assuming min_samples for DBSCAN is 90
min_samples = 40

# Initialize NearestNeighbors with n_neighbors set to min_samples + 1
neigh = NearestNeighbors(n_neighbors=min_samples + 1)
nbrs = neigh.fit(coords_in_radians)

# Find the k-nearest neighbors for each point
distances, indices = nbrs.kneighbors(coords_in_radians)

# Retrieve the distances to the min_samples-th nearest neighbor (90th nearest in this case)
distanceDec = sorted(distances[:, min_samples], reverse=True)

# Plotting the k-distance graph
plt.plot(list(range(1, len(distanceDec) + 1)), distanceDec)
plt.xlabel('Points')
plt.ylabel(f'Distance to {min_samples}th Nearest Neighbor')
plt.show()

from sklearn.metrics import silhouette_score

# Filter out noise points for silhouette score calculation
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_

# Number of clusters in labels, ignoring noise if present
n_clusters = len(set(labels)) - (1 if -1 in labels else 0)

# Ensure there are more than 1 clusters, excluding noise
if n_clusters > 1:
    # Calculate the silhouette score
    silhouette_avg = silhouette_score(coords_in_radians, labels)
    print('Silhouette Coefficient: %0.3f' % silhouette_avg)
else:
    print("Silhouette score cannot be calculated for a single cluster.")

"""*****HDBSCAN*****"""

import hdbscan

coords = data[['LAT', 'LON']]

# Apply HDBSCAN
# The min_cluster_size can be adjusted based on domain knowledge or experimentation
clusterer = hdbscan.HDBSCAN(min_cluster_size=40, metric='haversine')
data['cluster'] = clusterer.fit_predict(np.radians(coords))

# Handle noise points by assigning them to a separate cluster
data['cluster'] = data['cluster'] + 1  # shift cluster numbers up by one
data.loc[data['cluster'] == 0, 'cluster'] = -1  # reassign noise points to -1

# Count the number of crimes in each cluster, excluding noise points
crime_counts_per_cluster = data[data['cluster'] != -1].groupby('cluster').size()

# Define the risk levels
low_risk_threshold = crime_counts_per_cluster.quantile(0.25)
medium_risk_threshold = crime_counts_per_cluster.quantile(0.5)
high_risk_threshold = crime_counts_per_cluster.quantile(0.75)

# Assign a risk level to each cluster
def get_risk_level(count):
    if count <= low_risk_threshold:
        return 'Low Risk'
    elif count <= medium_risk_threshold:
        return 'Medium Risk'
    elif count <= high_risk_threshold:
        return 'High Risk'
    else:
        return 'Very High Risk'

data['risk_level'] = crime_counts_per_cluster.map(get_risk_level).reindex(data['cluster']).values

# Display the DataFrame with the cluster and risk level
print(data[['LAT', 'LON', 'cluster', 'risk_level']])

import matplotlib.pyplot as plt
from mpl_toolkits.basemap import Basemap
import numpy as np

# Assuming 'data' is your DataFrame and it contains a 'risk_level' column
# Map risk levels to numbers
risk_level_mapping = {'Low Risk': 0, 'Medium Risk': 1, 'High Risk': 2, 'Very High Risk': 3}
data['risk_level_num'] = data['risk_level'].map(risk_level_mapping)

# Set up the basemap with specific bounds
fig, ax = plt.subplots(figsize=(10, 15))
m = Basemap(projection='merc', llcrnrlat=data['LAT'].min()-0.01, urcrnrlat=data['LAT'].max()+0.01,
            llcrnrlon=data['LON'].min()-0.01, urcrnrlon=data['LON'].max()+0.01, lat_ts=20, resolution='i')

m.drawcoastlines()
m.drawcountries()
m.drawstates()
m.drawmapboundary(fill_color='aqua')
m.fillcontinents(color='orange', lake_color='aqua')

# Convert lat and lon to map projection coordinates
m_lons, m_lats = m(data['LON'].values, data['LAT'].values)

# Plot the points with colors based on risk level
scatter = ax.scatter(m_lons, m_lats, c=data['risk_level_num'], cmap='Set1', zorder=5)

# Create a colorbar and a legend
colorbar = plt.colorbar(scatter, ax=ax, fraction=0.03, pad=0.04)
colorbar.set_ticks(np.arange(0, 4))
colorbar.set_ticklabels(['Low', 'Medium', 'High', 'Very High'])
colorbar.set_label('Risk Level')

plt.title('HDBSCAN Clustering of Crimes by Risk Level')
plt.show()

"""**Color Coding:** Different colors represent various risk levels, ranging from low (light colors) to very high (dark colors). It appears that most areas have been classified into discrete clusters based on the density and proximity of crime incidents.

**Geographical Features:** The map seems to include a large body of water on the left, indicated by a light blue color, which might be a lake or a coastal area. This natural barrier could affect the distribution and concentration of crime in adjacent areas.

**Crime Risk Levels:**

**Red Clusters:** These areas are likely to be classified as 'Very High' risk zones. They are smaller in size but highly concentrated, indicating that they may be areas with a high density of crime incidents or hotspots requiring immediate attention.

**Purple Clusters:** Represent 'High' risk areas. These clusters are more spread out than the 'Very High' risk zones and may cover larger neighborhoods or regions with significant crime activity.

**Grey Clusters:** Denote 'Medium' risk levels. The distribution suggests that these areas experience a moderate level of crime that is less concentrated than in 'High' or 'Very High' risk zones.

**Brown Clusters:** Seem to signify 'Low' risk areas with sparse incidents of crime spread across larger areas.

**High-Risk Clusters Proximity to Water:** There seems to be a cluster of 'Very High' risk zones close to the water body. This could indicate that areas near the waterfront may be prone to higher crime rates, which might be due to higher population density, tourist activities, or other factors.

**Implications:** Law enforcement and city planners can use this map to identify areas of concern and allocate resources appropriately. For instance, 'Very High' risk areas may require increased policing and community intervention programs.

**Data-Driven Decisions:** The clustering results from HDBSCAN help in understanding the patterns of crime in relation to geographical locations, which is crucial for developing targeted crime prevention strategies.


"""

from sklearn.metrics import silhouette_score
from sklearn.metrics import calinski_harabasz_score

from sklearn.metrics import davies_bouldin_score

calinski_harabasz = calinski_harabasz_score(coords[data['cluster'] != -1],
                                            data[data['cluster'] != -1]['cluster'])

print('Calinski-Harabasz Index:', calinski_harabasz)


davies_bouldin = davies_bouldin_score(coords[data['cluster'] != -1],
                                      data[data['cluster'] != -1]['cluster'])

print('Davies-Bouldin Index:', davies_bouldin)

# Extract the coordinates as numpy array and radians for haversine distance


# Compute the silhouette score (only for non-noise clusters)
silhouette_avg = silhouette_score(coords[data['cluster'] != -1],
                                  data[data['cluster'] != -1]['cluster'])

print('Silhouette Score:', silhouette_avg)

"""***K-MEANS***"""

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt


# Assuming X is your data, for example:
X = data[['LAT', 'LON']].values

# Calculate WCSS for a range of k values
wcss = []
for i in range(2, 40): # test for k from 1 to 10
    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=500, n_init=10, random_state=0)
    kmeans.fit(X)
    wcss.append(kmeans.inertia_)

# Plot the results to observe 'The Elbow'
plt.figure(figsize=(12,6))
plt.plot(range(2, 40), wcss, marker='o')
plt.title('Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS') # Within-cluster sum of square
plt.xticks(range(1,11))
plt.show()

"""The elbow method plot is used to find the optimal number of clusters by fitting the k-means algorithm to the dataset for a range of values of k (the number of clusters). The graph plots the Within-Cluster-Sum-of-Squares (WCSS), which is the sum of squared distances between each point and the centroid of its cluster, against the number of clusters.

From the elbow method plot, we look for a point where the WCSS begins to decrease at a slower rate, creating an "elbow" in the graph. This point indicates that adding more clusters does not significantly improve the fit of the model. In the above plot, there appears to be an elbow at k=5, where the rate of decrease in WCSS slows down. This suggests that the optimal number of clusters for your dataset might be 5.
"""

import pandas as pd
from sklearn.cluster import KMeans
import numpy as np
from sklearn.preprocessing import StandardScaler


coords = data[['LAT', 'LON']]

optimal_clusters = 5
kmeans = KMeans(n_clusters=optimal_clusters, init='k-means++', max_iter=500, n_init=10, random_state=0)
pred_clusters = kmeans.fit_predict(coords)

data['Cluster'] = pred_clusters
cluster_risk = data.groupby('Cluster').size()

# Define risk levels based on the counts
risk_levels = pd.cut(cluster_risk, bins=[0, cluster_risk.quantile(0.25), cluster_risk.quantile(0.5), cluster_risk.quantile(0.75), float('inf')], labels=['Low', 'Medium', 'High', 'Very High'])
data['Risk_Level'] = data['Cluster'].map(risk_levels)

silhouette_avg = silhouette_score(coords, pred_clusters)
print(f'Silhouette Score for {optimal_clusters} clusters: {silhouette_avg:}')

davies_bouldin = davies_bouldin_score(coords, pred_clusters)
print(f'Davies-Bouldin Index: {davies_bouldin:}')

# Calculate Calinski-Harabasz Index
calinski_harabasz = calinski_harabasz_score(coords, pred_clusters)
print(f'Calinski-Harabasz Index: {calinski_harabasz:}')

import matplotlib.pyplot as plt
from mpl_toolkits.basemap import Basemap


plt.figure(figsize=(20, 15))

# Initialize the Basemap with the geographical bounds
m = Basemap(projection='merc',
            llcrnrlat=data['LAT'].min() - 1,
            urcrnrlat=data['LAT'].max() + 1,
            llcrnrlon=data['LON'].min() - 1,
            urcrnrlon=data['LON'].max() + 1,
            resolution='i')

# Draw coastlines, countries, and states
m.drawcoastlines()
m.drawcountries()

m.drawstates()

# Convert latitude and longitude to x and y coordinates
x, y = m(data['LON'].values, data['LAT'].values)

# Scatter plot with the cluster numbers as colors and different markers for each risk level
for risk_level in risk_levels.unique():
    idx = data['Risk_Level'] == risk_level
    m.scatter(x[idx], y[idx], zorder=5, label=str(risk_level), alpha=0.8, edgecolor='k', s=100, marker='o')

# Add a legend and title to the plot
plt.legend(loc='best')
plt.title('Crime Risk Levels by Location')

# plt.xlim(10, 20)
# plt.ylim(5, 15)
plt.show()

"""The visual analysis of the above plot reveals several clusters marked by different colors to represent varying levels of crime risk. It appears to be a hexbin plot or similar visualization that aggregates points into hexagonal bins, with each color corresponding to a specific risk category — low (green), medium (blue), high (orange), and very high (red).

Observations:

The largest concentration of high and very high crime risks is in one central cluster, suggesting a major urban area or city center with a higher crime rate.

The spread of medium and low-risk areas around the edges may indicate suburban or less densely populated areas where crime is less frequent.

The singular green dot separated from the main clusters might represent an outlier or a remote area with a notably low risk of crime.

The plot effectively illustrates how crime risk levels are not evenly distributed and tend to be higher in certain concentrated areas.
"""

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

silhouette_coefficients = []

# Notice you start at 2 clusters for silhouette coefficient
for i in range(2, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)
    kmeans.fit(X)
    score = silhouette_score(X, kmeans.labels_)
    silhouette_coefficients.append(score)

plt.figure(figsize=(10, 8))
plt.plot(range(2, 11), silhouette_coefficients, marker='o')
plt.title('Silhouette Coefficients For Optimal k')
plt.xlabel('Number of clusters')
plt.ylabel('Silhouette Coefficient')
plt.show()

"""The silhouette coefficient shows a higher value at k=2 compared to k=5, this indicates that with two clusters, the data points are, on average, more appropriately matched to their own cluster and more distinct from other clusters. However, if using only two clusters results in an overgeneralization of your data—such as having only "Very High" and "High" risk areas without finer granularity, we opted for a higher k value like 5, despite a potentially lower silhouette score.

Selecting k=5 gave a more nuanced segmentation, distinguishing regions into "Very High," "High," "Medium," and "Low" risk areas. It's a trade-off between the resolution of analysis and the clear separation of clusters.

****BONUS FUNCTIONALITY****

1. TEMPORAL ANALYSIS
2. CORRELATION WITH OTHER DATASET (ARREST DATA IN LA)
3. USING DATA FROM PLACES OTHER THAN LA (BUFFALO, NY)
"""

TEMPORAL ANALYSIS

df=data
# Create additional time-based columns
df['year'] = df['DATE OCC'].dt.year
df['month'] = df['DATE OCC'].dt.month
df['day'] = df['DATE OCC'].dt.day
df['hour'] = df['TIME OCC'].apply(lambda x: x.hour)

# Analyze crimes by year
crimes_by_year = df.groupby('year').size()

# Visualize the number of crimes each year
crimes_by_year.plot(kind='bar')
plt.title('Number of Crimes Each Year')
plt.xlabel('Year')
plt.ylabel('Number of Crimes')
plt.show()

# You can create similar visualizations for month, day, and hour.

# Analyze crimes by month
crimes_by_month = df.groupby('month').size()

# Visualize the number of crimes each month
crimes_by_month.plot(kind='bar')
plt.title('Number of Crimes Each Month')
plt.xlabel('Month')
plt.ylabel('Number of Crimes')
plt.show()

# Analyze crimes by day
crimes_by_year = df.groupby('day').size()

# Visualize the number of crimes each day
crimes_by_year.plot(kind='bar')
plt.title('Number of Crimes Each Day')
plt.xlabel('day')
plt.ylabel('Number of Crimes')
plt.show()

# Analyze crimes by hour
crimes_by_year = df.groupby('hour').size()

# Visualize the number of crimes each hour
crimes_by_year.plot(kind='bar')
plt.title('Number of Crimes Each Hour')
plt.xlabel('hour')
plt.ylabel('Number of Crimes')
plt.show()

"""**Yearly Crime Distribution:** The bar graph shows a relatively consistent number of crimes from 2020 to 2022, with a drastic drop in 2023, and an even more significant drop projected for 2024. The less crimes in the years 2020 and 2021 could be due to COVID-19. The sharp decline is due to incomplete data for the current year.

**Monthly Crime Distribution:** Crimes appear to be somewhat evenly distributed across months, with slight decreases in mid-year and increases toward the year's end. This could suggest seasonal patterns, possibly linked to social factors like holidays or weather conditions that may affect crime rates.

**Daily Crime Distribution:** The number of crimes reported each day varies, showing some peaks and troughs. It’s not entirely uniform, indicating that certain days may have higher crime occurrences, which could be analyzed further to understand daily patterns or specific events influencing these variations.

**Hourly Crime Distribution:** The distribution shows variability in crime occurrences throughout the day, with significant spikes during late evening and early morning hours. This might reflect the times when crimes are more likely to go unnoticed or when fewer preventative measures are active. The lower rates during the midday hours could be due to higher public presence and visibility, which deter criminal activities.
"""

from statsmodels.tsa.statespace.sarimax import SARIMAX
df=data
# Set the date of occurrence as the index
df.set_index('DATE OCC', inplace=True)

# Aggregate the data to get daily crime counts
daily_crimes = df.resample('D').size()

# The ARIMA model requires stationary data, so depending on the dataset you may need to difference the data
# For example, let's say we do a first-order differencing
daily_crimes_diff = daily_crimes.diff().dropna()

# Now you can fit a SARIMA model
# Note: You will need to determine the order and seasonal_order parameters based on your dataset
# Here's an example with placeholder parameters
sarima_model = SARIMAX(daily_crimes_diff, order=(3, 0, 1), seasonal_order=(1, 0, 1, 7))
sarima_result = sarima_model.fit()

# Now you can use the model to make predictions or understand the time series
sarima_result.summary()

# For pattern recognition, you might visualize the data
plt.figure(figsize=(10, 5))
plt.plot(daily_crimes_diff)
plt.title('Daily Crime Counts')
plt.xlabel('Date')
plt.ylabel('Number of Crimes')
plt.show()

# To identify variations in crime rates by time of day, week, and season,
# you would need to create new columns in your DataFrame for hour of the day, day of the week, etc.
# Then you could use groupby to aggregate and visualize these trends.

# For example, to look at crimes by day of the week:
df['day_of_week'] = df.index.dayofweek
weekly_crimes = df.groupby('day_of_week').size()

# Visualize
plt.figure(figsize=(10, 5))
weekly_crimes.plot(kind='bar')
plt.title('Crime Counts by Day of the Week')
plt.xlabel('Day of Week')
plt.ylabel('Number of Crimes')
plt.xticks(ticks=range(7), labels=['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'], rotation=45)
plt.show()

print(sarima_result.summary())
print("AIC:", sarima_result.aic)

from pmdarima import auto_arima


# Apply auto_arima function to the daily crime counts time series
sarima_model = auto_arima(daily_crimes_diff, seasonal=True, m=7,
                          start_p=0, start_q=0, start_P=0, start_Q=0,
                          max_p=3, max_q=3, max_P=3, max_Q=3,
                          trace=True, error_action='ignore',
                          suppress_warnings=True, stepwise=True)

print(sarima_model.summary())

"""CORRELATION WITH OTHER DATASETS: ARREST DATA IN LA"""

arrest_data = pd.read_csv('Arrest_Data_from_2020_to_Present_20240403.csv')

arrest_data.head()

arrest_data.info()

arrest_data.isnull().sum()

# Convert dates to datetime objects
arrest_data['Arrest Date'] = pd.to_datetime(arrest_data['Arrest Date'], errors='coerce')
arrest_data['Booking Date'] = pd.to_datetime(arrest_data['Booking Date'], errors='coerce')

# Handling missing values
# Drop rows where 'Time', 'Charge Group Code', and 'Charge Group Description' are missing
arrest_data = arrest_data.dropna(subset=['Time', 'Charge Group Code', 'Charge Group Description'])

# Fill missing 'Cross Street' with 'Unknown'
arrest_data['Cross Street'] = arrest_data['Cross Street'].fillna('Unknown')

# Dealing with outliers in 'Age' column
Q1 = arrest_data['Age'].quantile(0.25)
Q3 = arrest_data['Age'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Apply the caps
arrest_data = arrest_data[(arrest_data['Age'] >= lower_bound) & (arrest_data['Age'] <= upper_bound)]

# Convert data to appropriate data types
# Convert 'Area ID' to a categorical type as it's more of a label than a numeric value
arrest_data['Area ID'] = arrest_data['Area ID'].astype('category')

# Convert 'Sex Code', 'Descent Code', and 'Arrest Type Code' to category
arrest_data['Sex Code'] = arrest_data['Sex Code'].astype('category')
arrest_data['Descent Code'] = arrest_data['Descent Code'].astype('category')
arrest_data['Arrest Type Code'] = arrest_data['Arrest Type Code'].astype('category')

# Convert 'Report ID', 'Reporting District', 'Booking Location Code' to strings as they are identifiers
arrest_data['Report ID'] = arrest_data['Report ID'].astype(str)
arrest_data['Reporting District'] = arrest_data['Reporting District'].astype(str)
arrest_data['Booking Location Code'] = arrest_data['Booking Location Code'].astype(str)

# Save the cleaned data to a new CSV file
arrest_data.to_csv('cleaned_arrest_data.csv', index=False)

print("Data cleaning and preprocessing done. Saved to 'cleaned_arrest_data.csv'.")

crime_data=data
# Count the number of crimes by area
crime_counts = crime_data['AREA NAME'].value_counts()

# Count the number of arrests by area
arrest_counts = arrest_data['Area Name'].value_counts()

# Convert the counts to DataFrames
crime_counts_df = crime_counts.reset_index()
arrest_counts_df = arrest_counts.reset_index()

# Rename columns to prepare for merge
crime_counts_df.columns = ['Area Name', 'Crime Count']
arrest_counts_df.columns = ['Area Name', 'Arrest Count']

# Merge the crime and arrest data on Area Name
combined_data = pd.merge(crime_counts_df, arrest_counts_df, on='Area Name', how='outer')

# Calculate Crime to Arrest Ratio
combined_data['Crime to Arrest Ratio'] = combined_data['Crime Count'] / combined_data['Arrest Count']

# Replace infinite ratios with NaN if there are areas with arrests but no recorded crimes
combined_data.replace([np.inf, -np.inf], np.nan, inplace=True)

# Fill NaN values with a placeholder if you want (e.g., 0 or a string)
combined_data.fillna(0, inplace=True)

# Now you have a DataFrame with each area, their crime counts, arrest counts, and the ratio
print(combined_data)

"""The output suggests varying levels of crime and arrest rates across different areas:

**Areas with High Crime to Arrest Ratios:** Areas like Wilshire (1.70), West LA (1.65), Harbor (1.43), Northeast (1.39), and others with ratios above 1 show higher crime rates compared to arrest rates. This could indicate potential under-policing, higher crime occurrences, or perhaps inefficiencies in the arrest process.

**Areas with Crime to Arrest Ratios Around 1:** Areas such as Foothill (1.04) and Topanga (1.03) are closer to a 1:1 crime-to-arrest ratio. These areas may have a balance in the number of crimes reported and the arrests made, suggesting that law enforcement efforts might be effectively matching the crime rates.

**Areas with Low Crime to Arrest Ratios:** Areas like Hollywood (0.96), Rampart (0.83), and Central (0.93), where the ratios are below 1, indicate a higher arrest count compared to reported crimes. This could mean that these areas have more effective policing, or it might also indicate a focus on making arrests for smaller infractions.

**Area Disparities:** The difference in ratios across areas indicates disparities in law enforcement outcomes or possibly in crime reporting. Some areas seem to have more effective control over crime based on the number of arrests made relative to crimes reported.

**Policy Implications:** Areas with high crime-to-arrest ratios might need additional law enforcement resources or community intervention programs to address the high crime rates. Conversely, areas with low ratios might be models of effective policing strategies that could be studied and possibly replicated elsewhere.

**Ratio Interpretation:** A ratio greater than 1 does not necessarily indicate poor policing. It may also reflect a surge in crime reporting, a transient spike in crime, or a policy of caution in making arrests. Conversely, a ratio less than 1 doesn't always indicate effective policing, as it may also result from aggressive arrest policies that could include arrests for minor crimes.

**Areas with a Ratio Greater Than 1:** These areas have more reported crimes than arrests, which might suggest either a high incidence of crime, effective crime reporting, or potentially less effective policing or arrest rates in these areas.

**Areas with a Ratio Less Than 1:** In contrast, these areas have fewer reported crimes than arrests, indicating a higher arrest rate relative to the number of reported crimes. This could suggest more effective policing or lower crime rates.

**Close to 1 Ratio:** Areas where the ratio is around 1 have a balanced number of reported crimes to arrests, which might suggest that law enforcement is effectively managing the crime rate in these areas.
Significantly High or Low Ratios: Extremely high or low ratios might require further investigation. For example, a very high ratio could mean that many crimes go unresolved, while a very low ratio could indicate that the area is heavily policed or that many arrests are made for a relatively lower number of crimes.

USING DATASETS WITH LOCATION OTHER THAN LA
"""

ny = pd.read_csv('Crime_Incidents_20240403.csv')

ny.head()

ny.info()

ny.isnull().sum()

import pandas as pd
import numpy as np

# Load your dataset here
# ny = pd.read_csv('your_file.csv')

# Drop columns with all missing values
ny.dropna(axis=1, how='all', inplace=True)

# Convert 'Incident Datetime' to datetime objects
ny['Incident Datetime'] = pd.to_datetime(ny['Incident Datetime'], errors='coerce')

# Convert 'Latitude' and 'Longitude' to numeric and handle any non-numeric data that was entered
ny['Latitude'] = pd.to_numeric(ny['Latitude'], errors='coerce')
ny['Longitude'] = pd.to_numeric(ny['Longitude'], errors='coerce')

# Replace known placeholders for missing values with NaN
ny.replace({'[object Object]': pd.NA, 'UNKNOWN': pd.NA}, inplace=True)

# Assuming we want to fill numeric columns with the median
numeric_columns = ['Latitude', 'Longitude']
for col in numeric_columns:
    ny[col].fillna(ny[col].median(), inplace=True)

# For categorical data, we fill missing values with the most frequent value or a placeholder
categorical_columns = ['Address', 'City', 'State']
for col in categorical_columns:
    ny[col].fillna(ny[col].mode()[0], inplace=True)

# Assuming you have a 'Amount' column and you want to remove outliers from it
# If there is no such column, you can skip this part
if 'Amount' in ny.columns:
    Q1 = ny['Amount'].quantile(0.25)
    Q3 = ny['Amount'].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - (1.5 * IQR)
    upper_bound = Q3 + (1.5 * IQR)
    ny = ny[(ny['Amount'] >= lower_bound) & (ny['Amount'] <= upper_bound)]

# Change the data to appropriate datatypes
ny['Case Number'] = ny['Case Number'].astype(str)
ny['Incident Type Primary'] = ny['Incident Type Primary'].astype('category')
ny['Day of Week'] = ny['Day of Week'].astype('category')
ny['City'] = ny['City'].astype('category')
ny['State'] = ny['State'].astype('category')

# Convert zip codes, neighborhoods, and other identifiers to string to prevent any unintentional calculations
ny['zip_code'] = ny['zip_code'].astype(str)
ny['neighborhood'] = ny['neighborhood'].astype(str)

# Save the cleaned data to a new CSV file
ny.to_csv('cleaned_ny_data.csv', index=False)

print("Data preprocessing is complete. Cleaned data saved to 'cleaned_ny_data.csv'.")

import hdbscan

ny=ny[:10000]
coords = ny[['Latitude', 'Longitude']]

# Apply HDBSCAN
# The min_cluster_size can be adjusted based on domain knowledge or experimentation
clusterer = hdbscan.HDBSCAN(min_cluster_size=30, metric='haversine')
ny['cluster'] = clusterer.fit_predict(np.radians(coords))

# Handle noise points by assigning them to a separate cluster
ny['cluster'] = ny['cluster'] + 1  # shift cluster numbers up by one
ny.loc[ny['cluster'] == 0, 'cluster'] = -1  # reassign noise points to -1

# Count the number of crimes in each cluster, excluding noise points
crime_counts_per_cluster = ny[ny['cluster'] != -1].groupby('cluster').size()

# Define the risk levels
low_risk_threshold = crime_counts_per_cluster.quantile(0.25)
medium_risk_threshold = crime_counts_per_cluster.quantile(0.5)
high_risk_threshold = crime_counts_per_cluster.quantile(0.75)

# Assign a risk level to each cluster
def get_risk_level(count):
    if count <= low_risk_threshold:
        return 'Low Risk'
    elif count <= medium_risk_threshold:
        return 'Medium Risk'
    elif count <= high_risk_threshold:
        return 'High Risk'
    else:
        return 'Very High Risk'

ny['risk_level'] = crime_counts_per_cluster.map(get_risk_level).reindex(ny['cluster']).values

# Display the DataFrame with the cluster and risk level
print(ny[['Latitude', 'Longitude', 'cluster', 'risk_level']])

import matplotlib.pyplot as plt
from mpl_toolkits.basemap import Basemap
import numpy as np

# Assuming 'ny' is your DataFrame and it contains a 'risk_level' column
# Map risk levels to numbers
risk_level_mapping = {'Low Risk': 0, 'Medium Risk': 1, 'High Risk': 2, 'Very High Risk': 3}
ny['risk_level_num'] = ny['risk_level'].map(risk_level_mapping)

# Set up the basemap with specific bounds
fig, ax = plt.subplots(figsize=(10, 15))
m = Basemap(projection='merc', llcrnrlat=ny['Latitude'].min()-0.01, urcrnrlat=ny['Latitude'].max()+0.01,
            llcrnrlon=ny['Longitude'].min()-0.01, urcrnrlon=ny['Longitude'].max()+0.01, lat_ts=20, resolution='i')

m.drawcoastlines()
m.drawcountries()
m.drawstates()
m.drawmapboundary(fill_color='aqua')
m.fillcontinents(color='lightgreen', lake_color='aqua')

# Convert lat and lon to map projection coordinates
m_lons, m_lats = m(ny['Longitude'].values, ny['Latitude'].values)

# Plot the points with colors based on risk level
scatter = ax.scatter(m_lons, m_lats, c=ny['risk_level_num'], cmap='Set1', zorder=5)

# Create a colorbar and a legend
colorbar = plt.colorbar(scatter, ax=ax, fraction=0.03, pad=0.04)
colorbar.set_ticks(np.arange(0, 4))
colorbar.set_ticklabels(['Low', 'Medium', 'High', 'Very High'])
colorbar.set_label('Risk Level')

plt.title('Crime Risk Levels in NY')
plt.show()

"""We did the same analysis and modelling on the Crime Data in Buffalo, NY data, as we did on our original dataset.

We first performed data collection, data understanding, data preparation, and data analysis on the data.

Then we used the same model we trained on the LA dataset on this Buffalo dataset.

We used HDBSCAN to cluster the areas of Buffalo, NY into crime hotspots of low, medium, high and very high-risk areas.

The image below depicts a map overlaid with areas classified into different crime risk levels, based on an analysis conducted using HDBSCAN.

**Risk Levels Identified:** The map shows clusters of areas categorized into four risk levels—Low, Medium, High, and Very High—indicative of the relative crime frequency or severity in these regions.

**Geographic Spread:** The clustering presents a non-uniform distribution, with clusters of varying risk levels scattered throughout the map rather than being concentrated in a specific region.  

**Concentration of High Risk:** Several clusters marked as 'Very High' risk are noticeable. These are likely areas where a significantly higher number of crimes have been recorded. They are potential hotspots that may require more focused attention from law enforcement and community intervention programs.
"""

